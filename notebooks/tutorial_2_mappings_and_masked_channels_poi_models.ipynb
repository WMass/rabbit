{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d66795",
   "metadata": {},
   "source": [
    "# Tutorial 2: Mappings, masked channels, and POI models\n",
    "\n",
    "Masked channels are auxiliary channels that don't enter the fit in the default setup (allthough they might indirectly through more advanced use cases such as regularization). They contain processes with systematic variations as regular channels. This allows to propagate the fitresult into these distributions.\n",
    "\n",
    "A common use case is a cross section measurement. The cross section is not simply scaling with the signal strength modifier, i.e. $\\sigma_\\mathrm{measured} \\neq \\mu \\sigma$. Instead, it depends on all systematic uncertainties that change the cross section, usually the theory uncertainties. Masked channels allow to define histograms containing the full dependency of the cross section, i.e. $\\sigma_\\mathrm{measured} = \\sigma(\\mu, \\vec{\\theta})$ where $\\vec{\\theta}$ are all systematic uncertainties that the cross section depends on. In case a differential cross section measurement is performed, the masked channel contains the generator-level distribution wit all its dependencies.\n",
    "\n",
    "Mappings are transformations of parameters and observables (histograms). Baseline mappings are 'BaseMapping', 'Project', 'Ratio', or 'AngularCoefficients' but custom mappings can be defined. Any differential function can be implemented as a mapping. Mappings can be applied to regular or masked channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9b080",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18eb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RABBIT_BASE: /home/david/work/Repos/rabbit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Emulate RABBIT_BASE (points to the 'rabbit' root)\n",
    "# Since we are in rabbit/notebooks/, we go up one level\n",
    "rabbit_base = str(Path(os.getcwd()).parent.absolute())\n",
    "os.environ['RABBIT_BASE'] = rabbit_base\n",
    "\n",
    "# 2. Update PYTHONPATH so you can 'import rabbit'\n",
    "if rabbit_base not in sys.path:\n",
    "    sys.path.append(rabbit_base)\n",
    "\n",
    "# 3. Update PATH so you can run scripts from rabbit/bin/\n",
    "bin_path = os.path.join(rabbit_base, 'bin')\n",
    "if bin_path not in os.environ['PATH']:\n",
    "    os.environ['PATH'] = bin_path + os.pathsep + os.environ['PATH']\n",
    "\n",
    "print(f\"RABBIT_BASE: {os.environ['RABBIT_BASE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33882b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, IFrame, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd52fc",
   "metadata": {},
   "source": [
    "## Generating a synthetic toy model\n",
    "\n",
    "We generate the same model as in the tutorial 1, but now we an additional category axis, this could for example reflect 2 charges, lepton flavors, or any other variable. We also include a masked channel for the cross sections of the signal in these two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e5e9eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hist\n",
    "\n",
    "axis = hist.axis.Regular(10,0,1, name=\"x\")\n",
    "axis_cat = hist.axis.Regular(2,-2,2, name=\"cat\")\n",
    "\n",
    "# Create histograms for signal, 2 backhrounds, and data \n",
    "h_sig = hist.Hist(axis, axis_cat, storage=hist.storage.Weight())  \n",
    "h_flat = hist.Hist(axis, axis_cat, storage=hist.storage.Weight())  \n",
    "h_exp = hist.Hist(axis, axis_cat, storage=hist.storage.Weight())  \n",
    "h_data = hist.Hist(axis, axis_cat, storage=hist.storage.Double())  \n",
    "  \n",
    "# Generate and fill components with weights  \n",
    "np.random.seed(42)  \n",
    "# Gaussian signal (mean=0.5, std=0.1) with weights  \n",
    "sig_samples = np.random.normal(0.5, 0.1, 5000)\n",
    "sig_cats = np.random.choice([-1, 1], size=5000)\n",
    "sig_weights = np.random.normal(1.0, 0.2, 5000)  # Mean weight=1, sigma=0.2 \n",
    "h_sig.fill(sig_samples, sig_cats, weight=sig_weights)  \n",
    "  \n",
    "# Flat background with weights  \n",
    "flat_samples = np.random.uniform(0, 1, 4000)  \n",
    "flat_cats = np.random.choice([-1, 1], size=4000)\n",
    "flat_weights = np.random.normal(0.5, 0.1, 4000)  # Mean weight=0.5, sigma=0.1\n",
    "h_flat.fill(flat_samples, flat_cats, weight=flat_weights)  \n",
    "  \n",
    "# Exponential background with weights  \n",
    "exp_samples = np.random.exponential(0.2, 2000)  \n",
    "exp_cats = np.random.choice([-1, 1], size=2000)\n",
    "exp_weights = np.random.normal(1.5, 0.3, 2000)  # Mean weight=1.5, sigma=0.2 \n",
    "h_exp.fill(exp_samples, exp_cats, weight=exp_weights)  \n",
    "  \n",
    "# Sum components and add Poisson fluctuations\n",
    "h_data.values()[...] = np.random.poisson(  \n",
    "    h_sig.values() + h_flat.values() + h_exp.values()  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a3a3a",
   "metadata": {},
   "source": [
    "# Input tensor construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf3ee859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rabbit import tensorwriter  \n",
    "\n",
    "# 1. Initialize with the 2D channel\n",
    "writer = tensorwriter.TensorWriter(systematic_type=\"log_normal\")  \n",
    "\n",
    "# Note: axis and axis_cat were defined in your previous step\n",
    "writer.add_channel([axis, axis_cat], name=\"ch0\")\n",
    "\n",
    "# 2. Add Data and Processes (using the 2D histograms)\n",
    "writer.add_data(h_data, \"ch0\")  \n",
    "writer.add_process(h_flat, \"flat_bkg\", \"ch0\", signal=False)  \n",
    "writer.add_process(h_exp, \"exp_bkg\", \"ch0\", signal=False)  \n",
    "writer.add_process(h_sig, \"signal\", \"ch0\", signal=True)  \n",
    "\n",
    "# 3. Normalization Systematics (unchanged, applies to the whole volume)\n",
    "writer.add_norm_systematic(\"lumi\", [\"signal\", \"flat_bkg\", \"exp_bkg\"], \"ch0\", 1.01)\n",
    "writer.add_norm_systematic(\"flat_bkg_norm\", \"flat_bkg\", \"ch0\", 1.05)\n",
    "\n",
    "# 4. Generate and Add 2D Shape Systematics\n",
    "def make_shape_var(h, factor_func):  \n",
    "    h_var = h.copy()  \n",
    "    \n",
    "    # 1. Calculate the x-dependent weights (1D array)\n",
    "    centers_x = h.axes[0].centers  \n",
    "    weights_x = factor_func(centers_x)  \n",
    "    \n",
    "    # 2. Create the sign flip for the categories\n",
    "    # Assuming bin 0 is 'negative' and bin 1 is 'positive'\n",
    "    # This creates an array: [-1, 1]\n",
    "    category_signs = np.array([-1, 1])\n",
    "    \n",
    "    # 3. Combine them using broadcasting\n",
    "    # (N, 1) * (1, 2) results in an (N, 2) matrix of weights\n",
    "    # The first column will be -weights_x, the second will be +weights_x\n",
    "    total_weights = weights_x[:, np.newaxis] * category_signs[np.newaxis, :]\n",
    "        \n",
    "    # 4. Apply to the histogram values\n",
    "    h_var.values()[...] = h.values() * (1 + total_weights)  \n",
    "    return h_var\n",
    "\n",
    "# Variation for flat background\n",
    "h_flat_up = make_shape_var(h_flat, lambda c: 0.05 * np.exp(2 * (c - 0.5)))  \n",
    "h_flat_dn = make_shape_var(h_flat, lambda c: -0.03 * np.exp(2 * (c - 0.5)))  \n",
    "\n",
    "writer.add_systematic(  \n",
    "    [h_flat_up, h_flat_dn],  \n",
    "    \"flat_bkg_shape\",  \n",
    "    \"flat_bkg\",  \n",
    "    \"ch0\",  \n",
    "    symmetrize=\"linear\",\n",
    "    constrained=True,  \n",
    ")  \n",
    "\n",
    "# Variation for signal (slope)\n",
    "h_sig_up = make_shape_var(h_sig, lambda c: 0.5 * (c - 0.5))  \n",
    "h_sig_dn = make_shape_var(h_sig, lambda c: -0.5 * (c - 0.5))  \n",
    "\n",
    "writer.add_systematic(  \n",
    "    [h_sig_up, h_sig_dn],  \n",
    "    \"slope\",  \n",
    "    \"signal\",  \n",
    "    \"ch0\",  \n",
    "    symmetrize=\"average\",  \n",
    "    constrained=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c60a7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a masked channel\n",
    "#   for simplicity we are skipping convolution effect between cross sections (gen-level) \n",
    "#   and fitted detector-level distributions\n",
    "\n",
    "h_sig_xsec = hist.Hist(axis_cat, storage=hist.storage.Weight(), data=h_sig.project(\"cat\"))\n",
    "\n",
    "writer.add_channel([axis_cat], name=\"ch0_masked\", masked=True)\n",
    "writer.add_process(h_sig_xsec, \"signal\", \"ch0_masked\", signal=True)\n",
    "\n",
    "writer.add_systematic(  \n",
    "    [h_sig_up.project(\"cat\"), h_sig_dn.project(\"cat\")],  \n",
    "    \"slope\",  \n",
    "    \"signal\",  \n",
    "    \"ch0_masked\",  \n",
    "    symmetrize=\"average\",  \n",
    "    constrained=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecee2fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory=\"results/mappings_and_masked_channels/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Write the input tensor to HDF5  \n",
    "writer.write(\n",
    "    outfilename=f\"{directory}/input.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42979b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cde7eba",
   "metadata": {},
   "source": [
    "### Masked channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06778619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f603c66a",
   "metadata": {},
   "source": [
    "## Input data diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5a538",
   "metadata": {},
   "source": [
    "Plot inputdata\n",
    "Debug inputdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bb51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac645cb2",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db94378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4345cd28",
   "metadata": {},
   "source": [
    "## Fit Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9574185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b51179",
   "metadata": {},
   "source": [
    "### Prefit & postfit plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c06dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "296d1ecf",
   "metadata": {},
   "source": [
    "### Pulls and impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282ea08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb8c3a3",
   "metadata": {},
   "source": [
    "### Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d7efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98683557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819aed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120c7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14c7ae8c",
   "metadata": {},
   "source": [
    "## Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c11c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b26eaa9",
   "metadata": {},
   "source": [
    "### Baseline mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff294c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642d36e3",
   "metadata": {},
   "source": [
    "### Custom mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd913f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "358b2ca4",
   "metadata": {},
   "source": [
    "## POI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25955c99",
   "metadata": {},
   "source": [
    "### Baseline POI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b535bfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc76de43",
   "metadata": {},
   "source": [
    "### Custom POI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b8907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Rabbit)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
